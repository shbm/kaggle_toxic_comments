{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_raw = pd.read_csv(\"./train_kaggle_toxic_comments.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_raw.drop(['id'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = data_raw\n",
    "df['total_length'] = df['comment_text'].apply(len)\n",
    "df['capitals'] = df['comment_text'].apply(lambda comment: sum(1 for c in comment if c.isupper()))\n",
    "df['caps_vs_length'] = df.apply(lambda row: float(row['capitals'])/float(row['total_length']),\n",
    "                                axis=1)\n",
    "df['num_exclamation_marks'] = df['comment_text'].apply(lambda comment: comment.count('!'))\n",
    "df['num_question_marks'] = df['comment_text'].apply(lambda comment: comment.count('?'))\n",
    "df['num_punctuation'] = df['comment_text'].apply(\n",
    "    lambda comment: sum(comment.count(w) for w in '.,;:'))\n",
    "df['num_symbols'] = df['comment_text'].apply(\n",
    "    lambda comment: sum(comment.count(w) for w in '*&$%'))\n",
    "df['num_words'] = df['comment_text'].apply(lambda comment: len(comment.split()))\n",
    "df['num_unique_words'] = df['comment_text'].apply(\n",
    "    lambda comment: len(set(w for w in comment.split())))\n",
    "df['words_vs_unique'] = df['num_unique_words'] / df['num_words']\n",
    "df['num_smilies'] = df['comment_text'].apply(\n",
    "    lambda comment: sum(comment.count(w) for w in (':-)', ':)', ';-)', ';)')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = ('total_length', 'capitals', 'caps_vs_length', 'num_exclamation_marks',\n",
    "            'num_question_marks', 'num_punctuation', 'num_words', 'num_unique_words',\n",
    "            'words_vs_unique', 'num_smilies', 'num_symbols')\n",
    "columns = ('toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate')\n",
    "\n",
    "rows = [{c:df[f].corr(df[c]) for c in columns} for f in features]\n",
    "df_correlations = pd.DataFrame(rows, index=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>insult</th>\n",
       "      <th>obscene</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>threat</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>total_length</th>\n",
       "      <td>-0.013647</td>\n",
       "      <td>-0.045052</td>\n",
       "      <td>-0.042945</td>\n",
       "      <td>0.010131</td>\n",
       "      <td>-0.008011</td>\n",
       "      <td>-0.054470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>capitals</th>\n",
       "      <td>0.053576</td>\n",
       "      <td>0.075945</td>\n",
       "      <td>0.081691</td>\n",
       "      <td>0.143410</td>\n",
       "      <td>0.033581</td>\n",
       "      <td>0.091175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>caps_vs_length</th>\n",
       "      <td>0.093726</td>\n",
       "      <td>0.170384</td>\n",
       "      <td>0.182452</td>\n",
       "      <td>0.169243</td>\n",
       "      <td>0.055596</td>\n",
       "      <td>0.220777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_exclamation_marks</th>\n",
       "      <td>0.006005</td>\n",
       "      <td>0.027010</td>\n",
       "      <td>0.024362</td>\n",
       "      <td>0.060578</td>\n",
       "      <td>0.034202</td>\n",
       "      <td>0.037337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_question_marks</th>\n",
       "      <td>-0.000516</td>\n",
       "      <td>0.004859</td>\n",
       "      <td>0.005404</td>\n",
       "      <td>-0.004541</td>\n",
       "      <td>-0.003949</td>\n",
       "      <td>0.027820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_punctuation</th>\n",
       "      <td>-0.021698</td>\n",
       "      <td>-0.049487</td>\n",
       "      <td>-0.044119</td>\n",
       "      <td>-0.010763</td>\n",
       "      <td>-0.003723</td>\n",
       "      <td>-0.056714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_words</th>\n",
       "      <td>-0.014493</td>\n",
       "      <td>-0.043642</td>\n",
       "      <td>-0.042207</td>\n",
       "      <td>0.008452</td>\n",
       "      <td>-0.006688</td>\n",
       "      <td>-0.052444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_unique_words</th>\n",
       "      <td>-0.032796</td>\n",
       "      <td>-0.080960</td>\n",
       "      <td>-0.080942</td>\n",
       "      <td>-0.048377</td>\n",
       "      <td>-0.020279</td>\n",
       "      <td>-0.096256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>words_vs_unique</th>\n",
       "      <td>0.010632</td>\n",
       "      <td>0.043348</td>\n",
       "      <td>0.042755</td>\n",
       "      <td>-0.027050</td>\n",
       "      <td>-0.004020</td>\n",
       "      <td>0.056491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_smilies</th>\n",
       "      <td>0.001206</td>\n",
       "      <td>-0.004066</td>\n",
       "      <td>-0.003254</td>\n",
       "      <td>-0.002564</td>\n",
       "      <td>-0.001524</td>\n",
       "      <td>-0.003833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_symbols</th>\n",
       "      <td>0.006533</td>\n",
       "      <td>0.000568</td>\n",
       "      <td>0.004799</td>\n",
       "      <td>0.006697</td>\n",
       "      <td>0.000841</td>\n",
       "      <td>0.007708</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       identity_hate    insult   obscene  severe_toxic  \\\n",
       "total_length               -0.013647 -0.045052 -0.042945      0.010131   \n",
       "capitals                    0.053576  0.075945  0.081691      0.143410   \n",
       "caps_vs_length              0.093726  0.170384  0.182452      0.169243   \n",
       "num_exclamation_marks       0.006005  0.027010  0.024362      0.060578   \n",
       "num_question_marks         -0.000516  0.004859  0.005404     -0.004541   \n",
       "num_punctuation            -0.021698 -0.049487 -0.044119     -0.010763   \n",
       "num_words                  -0.014493 -0.043642 -0.042207      0.008452   \n",
       "num_unique_words           -0.032796 -0.080960 -0.080942     -0.048377   \n",
       "words_vs_unique             0.010632  0.043348  0.042755     -0.027050   \n",
       "num_smilies                 0.001206 -0.004066 -0.003254     -0.002564   \n",
       "num_symbols                 0.006533  0.000568  0.004799      0.006697   \n",
       "\n",
       "                         threat     toxic  \n",
       "total_length          -0.008011 -0.054470  \n",
       "capitals               0.033581  0.091175  \n",
       "caps_vs_length         0.055596  0.220777  \n",
       "num_exclamation_marks  0.034202  0.037337  \n",
       "num_question_marks    -0.003949  0.027820  \n",
       "num_punctuation       -0.003723 -0.056714  \n",
       "num_words             -0.006688 -0.052444  \n",
       "num_unique_words      -0.020279 -0.096256  \n",
       "words_vs_unique       -0.004020  0.056491  \n",
       "num_smilies           -0.001524 -0.003833  \n",
       "num_symbols            0.000841  0.007708  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         Explanation\\nWhy the edits made under my usern...\n",
       "1         D'aww! He matches this background colour I'm s...\n",
       "2         Hey man, I'm really not trying to edit war. It...\n",
       "3         \"\\nMore\\nI can't make any real suggestions on ...\n",
       "4         You, sir, are my hero. Any chance you remember...\n",
       "5         \"\\n\\nCongratulations from me as well, use the ...\n",
       "6              COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK\n",
       "7         Your vandalism to the Matt Shirvington article...\n",
       "8         Sorry if the word 'nonsense' was offensive to ...\n",
       "9         alignment on this subject and which are contra...\n",
       "10        \"\\nFair use rationale for Image:Wonju.jpg\\n\\nT...\n",
       "11        bbq \\n\\nbe a man and lets discuss it-maybe ove...\n",
       "12        Hey... what is it..\\n@ | talk .\\nWhat is it......\n",
       "13        Before you start throwing accusations and warn...\n",
       "14        Oh, and the girl above started her arguments w...\n",
       "15        \"\\n\\nJuelz Santanas Age\\n\\nIn 2002, Juelz Sant...\n",
       "16        Bye! \\n\\nDon't look, come or think of comming ...\n",
       "17         REDIRECT Talk:Voydan Pop Georgiev- Chernodrinski\n",
       "18        The Mitsurugi point made no sense - why not ar...\n",
       "19        Don't mean to bother you \\n\\nI see that you're...\n",
       "20        \"\\n\\n Regarding your recent edits \\n\\nOnce aga...\n",
       "21        \"\\nGood to know. About me, yeah, I'm studying ...\n",
       "22        \"\\n\\n Snowflakes are NOT always symmetrical! \\...\n",
       "23        \"\\n\\n The Signpost: 24 September 2012 \\n\\n Rea...\n",
       "24        \"\\n\\nRe-considering 1st paragraph edit?\\nI don...\n",
       "25        Radial symmetry \\n\\nSeveral now extinct lineag...\n",
       "26        There's no need to apologize. A Wikipedia arti...\n",
       "27        Yes, because the mother of the child in the ca...\n",
       "28        \"\\nOk. But it will take a bit of work but I ca...\n",
       "29        \"== A barnstar for you! ==\\n\\n  The Real Life ...\n",
       "                                ...                        \n",
       "153134    \" \\n\\n == Same coffee shop? == \\n\\n My memory ...\n",
       "153135    SO many things wrong with that viewpoint - fro...\n",
       "153136    \" \\n\\n Unless we have an article for some othe...\n",
       "153137    Hannah and Maddie are soooooo awesome and are ...\n",
       "153138    :::no problem, I tagged it and cleaned it out....\n",
       "153139    :PS I've just looked at the history of this ar...\n",
       "153140    \"== Your edit to Maungaturoto == \\n Please don...\n",
       "153141    :If you wish to contest the prod, please remov...\n",
       "153142    Balancing the two approaches to psychiatry ( b...\n",
       "153143                                   Ah, suck my balls.\n",
       "153144    == Your name mentioned == \\n Hi, I just though...\n",
       "153145    I've just discovered yet another list: List of...\n",
       "153146    ==Wikiproject Video Games assessment== \\n I do...\n",
       "153147    ::Consensus for ruining Wikipedia? I think tha...\n",
       "153148    == DAP ?  == \\n\\n What's point with DAP ?! Naz...\n",
       "153149    shut down the mexican border withought looking...\n",
       "153150    :Jerome, I see you never got around to thisâ€¦! ...\n",
       "153151    ==Lucky bastard== \\n http://wikimediafoundatio...\n",
       "153152    ==WTF== \\n It's no longer a redlink.  Now what...\n",
       "153153    \" \\n\\n ==\"\"Illness\"\" no shit== \\n Just for the...\n",
       "153154    ==shame on you all!!!== \\n\\n You want to speak...\n",
       "153155    MEL GIBSON IS A NAZI BITCH WHO MAKES SHITTY MO...\n",
       "153156    \" \\n\\n == Unicorn lair discovery == \\n\\n Suppo...\n",
       "153157    :Disagree. Soviet railways need their own arti...\n",
       "153158    This idiot can't even use proper grammar when ...\n",
       "153159    . \\n i totally agree, this stuff is nothing bu...\n",
       "153160    == Throw from out field to home plate. == \\n\\n...\n",
       "153161    \" \\n\\n == Okinotorishima categories == \\n\\n I ...\n",
       "153162    \" \\n\\n == \"\"One of the founding nations of the...\n",
       "153163    \" \\n :::Stop already. Your bullshit is not wel...\n",
       "Name: comment_text, Length: 312735, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from scipy.sparse import hstack\n",
    "from scipy.special import logit, expit\n",
    "from tqdm import tqdm\n",
    "\n",
    "class_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "train = pd.read_csv('./train_kaggle_toxic_comments.csv').fillna(' ')\n",
    "test = pd.read_csv('./test_kaggle_toxic_comments.csv').fillna(' ')\n",
    "\n",
    "train_text = train['comment_text']\n",
    "test_text = test['comment_text']\n",
    "all_text = pd.concat([train_text, test_text])\n",
    "\n",
    "all_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    token_pattern=r'\\w{1,}',\n",
    "    ngram_range=(1, 1),\n",
    "    max_features=15000)\n",
    "word_vectorizer.fit(all_text)\n",
    "train_word_features = word_vectorizer.transform(train_text)\n",
    "test_word_features = word_vectorizer.transform(test_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-7ecf7e2ee11b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mngram_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     max_features=20000)\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mchar_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mtrain_char_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchar_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mtest_char_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchar_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1330\u001b[0m         \u001b[0mself\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m         \"\"\"\n\u001b[0;32m-> 1332\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1333\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1334\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m--> 839\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m    840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    782\u001b[0m                                  \" contain stop words\")\n\u001b[1;32m    783\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 784\u001b[0;31m         \u001b[0mj_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    785\u001b[0m         \u001b[0mindptr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrombuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindptr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfrombuffer_empty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \"\"\"\n\u001b[0;32m--> 531\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "char_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='char',\n",
    "    ngram_range=(1, 5),\n",
    "    max_features=20000)\n",
    "char_vectorizer.fit(all_text)\n",
    "train_char_features = char_vectorizer.transform(train_text)\n",
    "test_char_features = char_vectorizer.transform(test_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_features = hstack([train_char_features, train_word_features])\n",
    "test_features = hstack([test_char_features, test_word_features])\n",
    "\n",
    "losses = []\n",
    "predictions = {'id': test['id']}\n",
    "for class_name in tqdm(class_names):\n",
    "    train_target = train[class_name]\n",
    "    classifier = LogisticRegression(solver='sag')\n",
    "\n",
    "    cv_loss = np.mean(cross_val_score(classifier, train_features, train_target, cv=3, scoring='roc_auc'))\n",
    "    losses.append(cv_loss)\n",
    "    print('CV score for class {} is {}'.format(class_name, cv_loss))\n",
    "\n",
    "    classifier.fit(train_features, train_target)\n",
    "    predictions[class_name] = classifier.predict_proba(test_features)[:, 1]\n",
    "\n",
    "print('Total CV score is {}'.format(np.mean(losses)))\n",
    "\n",
    "submission = pd.DataFrame.from_dict(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from scipy.sparse import hstack\n",
    "from scipy.special import logit, expit\n",
    "from tqdm import tqdm\n",
    "\n",
    "class_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "train = pd.read_csv('./train_kaggle_toxic_comments.csv').fillna(' ')\n",
    "test = pd.read_csv('./test_kaggle_toxic_comments.csv').fillna(' ')\n",
    "\n",
    "train_text = train['comment_text']\n",
    "test_text = test['comment_text']\n",
    "all_text = pd.concat([train_text, test_text])\n",
    "\n",
    "word_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    token_pattern=r'\\w{1,}',\n",
    "    ngram_range=(1, 1),\n",
    "    max_features=15000)\n",
    "word_vectorizer.fit(all_text)\n",
    "train_word_features = word_vectorizer.transform(train_text)\n",
    "test_word_features = word_vectorizer.transform(test_text)\n",
    "\n",
    "char_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='char',\n",
    "    ngram_range=(1, 5),\n",
    "    max_features=20000)\n",
    "char_vectorizer.fit(all_text)\n",
    "train_char_features = char_vectorizer.transform(train_text)\n",
    "test_char_features = char_vectorizer.transform(test_text)\n",
    "\n",
    "train_features = hstack([train_char_features, train_word_features])\n",
    "test_features = hstack([test_char_features, test_word_features])\n",
    "\n",
    "losses = []\n",
    "predictions = {'id': test['id']}\n",
    "for class_name in tqdm(class_names):\n",
    "    train_target = train[class_name]\n",
    "    classifier = LogisticRegression(solver='sag')\n",
    "\n",
    "    cv_loss = np.mean(cross_val_score(classifier, train_features, train_target, cv=3, scoring='roc_auc'))\n",
    "    losses.append(cv_loss)\n",
    "    print('CV score for class {} is {}'.format(class_name, cv_loss))\n",
    "\n",
    "    classifier.fit(train_features, train_target)\n",
    "    predictions[class_name] = classifier.predict_proba(test_features)[:, 1]\n",
    "\n",
    "print('Total CV score is {}'.format(np.mean(losses)))\n",
    "\n",
    "submission = pd.DataFrame.from_dict(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_score(votes, item_hour_age, gravity=1.8):\n",
    "    return (votes - 1) / pow((item_hour_age+2), gravity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.randint(1, 15+1, size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5,  3,  5,  3,  2,  9,  1, 10,  3, 13])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
